# imports

This directory provides a zip file containing Python scripts and example data files that can be used to generate batches of wiki pages to import into Mediawiki. The scripts were developed for By The Sword Linked but can be adapted for other wikis.

## Directory structure

- **imports/**
  - **merge in/** Directory where XML files generated by 'import.py' are usually saved. Contains some example data.
  - **merge out/** Directory where XML files generated by 'xml split merge.py' are usually saved. Contains some example data.
  - **redirects/** Directory for input and output files for 'makeredirects.py'. Contains some example data.
  - **settings/** Directory containing configuration files for scripts.
  - **Templates/** Directory for input data for 'import.py'. Should have a subdirectory for each main wiki template. The directory name must match the template name exactly, including spaces and capitalization. Each template directory should have a subdirectory for each batch of pages. Batch directory names must be unique for all batches that are to be imported together, even if they use different templates. Some example data is included here, but this does not cover all templates that are used in By The Sword Linked, or all batches that have been imported.
  - **import.py** Script to generate wikitext XML files from delimited text files.
  - **makefreetext.py** Script to generate a file in which free text can be manually entered for each page. This text will be automatically added to XML files generated by 'import.py'.
  - **wikifunctions.py** Functions used by other scripts.
  - **xml split merge.py** Script that can split or merge wikitext XML files to make larger or smaller files.

All files are encoded as utf-8 with UNIX line endings.

## Configuration

This process is designed for a wiki in which a page represents an entity and uses a main template for each entity type. It is particularly suited to Semantic Mediawiki, but this is not required. It can accomodate nested subtemplates.

The 'settings/' directory contains these files, which may need to be edited to work with a wiki other than By The Sword Linked:

- 'namespaces.txt' is a list of main templates that should not be used in the main namespace, and the ID number of the wiki namespace that pages using that template should be in. Separated by semi-colon, not tab. If a template is not listed, the script will default to 0 (Main).
* 'subtemplates nested.txt' lists the parameter that a subtemplate is nested in, then the name of the subtemplate. Separated by semi-colon, not tab. Parameter names are not qualified by the name of the main template that they are used in, so make sure there are no conflicts.

## Creating delimited text files

The scripts that generate wikitext XML need structured data in delimited text files. These can be created manually in a spreadsheet, or generated by some other method, such as Open Refine or a custom script of your own.

General requirements of these files are:

- extension: .csv
- separator: TAB
- line endings: UNIX
- character encoding: utf-8
- field values should never be enclosed by quotation marks or any other delimiter

### Delimited text for entity pages

A wiki page representing an entity always needs a main template of the correct type. Some of these templates can also include repeatable subtemplates, but some don't.

The data for the main template should be saved as 'main.csv' in the subdirectory for the batch. Batch directory names must be unique for all batches that are to be imported together, even if they use different templates.

If subtemplates are needed, the data for each type of subtemplate should be saved in the same directory as the main file. The filename for subtemplates should be the name of the template followed by '.csv'. This should exactly match how it appears in the wiki, including spaces and capitalization. It should not be URL encoded.

For main templates and subtemplates, the first row should contain the names of the template parameters. Again these should exactly match how they appear in the wiki, including spaces and capitalization.

The first column is usually called 'PageName', although it could be called anything as it does not have to match a template parameter. This column contains the wiki page name for each entity. These should exactly match how they appear in the wiki, including spaces and capitalization, and should not be URL encoded. The script will fail and give an error if a subtemplate file includes a page name that is not in the main file. Entities in the main file do not have to have any subtemplates, even if some entities in the batch have them.

All other columns are values for the template parameters named in the first row. The file does not have to include all parameters that a template can use. Cells do not have to have a value. If there is no value, that parameter will be skipped for that entity.

In By The Sword Linked, some template parameters can have multiple values separated by a semi-colon. In this case, all values and separators should be entered in the same cell.

Geographic coordinates should be in decimal degrees. The Maps extension for Semantic Mediawiki assumes that coordinates use WGS84. If your coordinates use a different datum, they will refer to different points on Earth even though the numbers look the same.

#### Manually creating delimited text for entity pages

Delimited text files can be created manually in a spreadsheet. For By The Sword Linked, the usual method is to create a spreadsheet file in the batch subdirectory with a worksheet for each template. For convenience, the worksheets can be given names that match the delimited text file name for each template.

An empty spreadsheet containing all the correct parameter names can be kept in the subdirectory for each template, although that has not been done in the example data.

#### Free text for entity pages

Free text can be added to wiki pages after the main template. This should be kept in a separate text file named 'free.txt' in the same subdirectory as the rest of the files for the batch. This file should contain each page name prefixed by '<<' and followed by '>>', then any text to be inserted after the main template. In By The Sword Linked, headings should be specified in this file, including wiki markup for headings. These will typically be level 2 headings 'Links to sources' or 'Any other information'.

The script 'makefreetext.py' can automatically generate a file with names of all pages in a batch using the page names from 'main.csv'. The script parameters need to specify the template and batch name only. The resulting file will need free text adding manually.

The 'free.txt' file could also be generated automatically by other means but these will have to be worked out on a case by case basis.

### Delimited text for redirects

Batches of redirects can be created for import into MediaWiki. The process is simpler than creating entity pages with templates.

A delimited text file should be saved in the directory 'redirects/in/'. It should have the following fields, with column names in the first row:

| Column name | Description |
| --- | --- |
| RedirFrom | The name of the page where the redirect is stored. |
| RedirTo | The page name that is the target of the redirect. After import, following a link in the wiki to RedirFrom will take the user to RedirTo |
| SortBy | Optional. A default sort key, if the redirect needs to be sorted in a different order from its page name. For example, if a redirect is to be categorized and has a special character in its page name that would put it in an unusual order. |
| Cats | Optional. One or more wiki categories to add the redirect page to. Multiple values should be separated by a semi-colon. Include only the name of the category, and not the 'Category:' prefix. |

While the resulting file must be saved in 'redirects/in/', a working spreadsheet used to create redirects manually can be saved anywhere. In By The Sword Linked, it was often convenient to save a redirect worksheet in the same spreadsheet file as the template data. If this is done, remember to save the redirects csv file in 'redirects/in/', not in the batch directory with the rest of the files.

## Generating wikitext XML

Once delimited text files have been created, by whatever method, the Python scripts can be used to convert them into wikitext XML which can be imported into MediaWiki.

Each of these scripts needs to be manually edited to set parameters. These parameters are documented in comments in the source files, and under the headings below. There is no GUI.

### Generating XML for entity pages

XML for entity pages is made by 'import.py'.

Before running this script, make sure that all necessary csv files exist in the correct directory for the template and batch. If 'free.txt' is used, this must also exist in the same directory before running 'import.py'.

Script parameters:

| Variable name | Variable type | Description |
| --- | --- | --- |
| templatename | string | Name of main template. Should be exactly the same as name of a wiki template and a subdirectory of 'Templates/', including spaces and capitalization. |
| batchname | string | Name for the batch. Can be anything but must have a matching subdirectory inside the Template directory, and must be unique among all batches to be imported at the same time, even if they use different templates. Will be used for part of the output file name. |
| maxpages | integer | Maximum number of pages per XML file. If the total number of pages in the batch exceeds this number, it will be saved as more than one XML file. The size of XML files can be changed further using the script 'xml split merge.py' |
| usernumber | string | Wiki user number. Edits will be attributed to this user. Needs to be string because will be written into XML file. |
| username | string | Wiki user name. Edits will be attributed to this user. |
| commentstr | string | Edit summary. This will be displayed in page history. |

Make sure that strings are surrounded by quotation marks and that integers are not.

When the script is run, it will print the name of each page to the screen as it is processed. If there are no errors, the output file will be saved in 'merge in/'. The filename will be prefixed with 'aaa-' to make it easier to find. This file can be imported to a wiki as it is, or it can be combined with other files using 'xml split merge.py'.

### Generating XML for redirects

XML for redirect pages is made by 'makeredirects.py'.

Before running this script, make sure that at least one correctly formatted csv file exists in the directory 'redirects/in/.

 Script parameters:

| Variable name | Variable type | Description |
| --- | --- | --- |
| batchname | string | Name for the batch. Can be anything but must be different from any other batch of pages that will be imported at the same time. Only used for part of the output file name. |
| maxpages | integer | Maximum number of pages per XML file. If the total number of pages in the batch exceeds this number, it will be saved as more than one XML file. The size of XML files can be changed further using the script 'xml split merge.py'. |
| usernumber | string | Wiki user number. Edits will be attributed to this user. Needs to be string because will be written into XML file. |
| username | string | Wiki user name. Edits will be attributed to this user. |

Make sure that strings are surrounded by quotation marks and that integers are not.

This script does not add an edit summary to the page history.

The output XML file will be written to 'merge/in/', not to the 'redirects/' directory. The filename will be prefixed with 'aaa-' to make it easier to find. This file can be imported to a wiki as it is, or it can be combined with other files using 'xml split merge.py'.

### Splitting and merging XML

The script 'xml split merge.py' can combine multiple wikitext XML files into a bigger file, or split a big file into a smaller file. This works on files generated by 'import.py' and 'makeredirects.py', and on files exported from a wiki using Special:Export or the maintenance script 'dumpBackup.php'.

Script parameters:

| Variable name | Variable type | Description |
| --- | --- | --- |
| batchname | string | Base name for XML output files. Can be anything. Not used for locating directories. Filename will have the prefix 'aaa-', then the value of batchname as a further prefix, then a zero filled number. |
| maxpages | integer | Maximum number of pages per output file. |
| inpath | string | Path to the directory where input files are. Default value is 'merge in' and does not usually need to be changed. |
| outpath | string | Path to the directory where output files will be saved. Default value is 'merge out' and does not usually need to be changed. |

## Importing XML to wiki

Importing the XML files to a wiki will normally use the maintenance script 'importDump.php'. This requires SSH access. The XML files are also compatible with Special:Import but this can only import a limited number of pages and doesn't always work properly. It may still require running maintenance scripts from the command line to make sure the database is properly updated.

Upload the XML file to the 'maintenance/' directory of your MediaWiki installation. This can be done by any method that you have access to, such as FTP or the cPanel file manager.

Enable SSH. Refer to your web host's documentation for instructions on how to do this. With Krystal hosting, it is done in the billing area, not in cPanel. It's safest to only enable SSH immediately before you use it, and disable it as soon as you finish.

Open an SSH connection in the terminal. By The Sword Linked used Linux. In Windows, you may need to install an extra application before you can make an SSH connection through the command line.

The usual command is as follows, but this may vary between hosts:

    ssh -p 722 [username]@[domain name]

You will be asked for a password, which you can type or paste in. It may not appear on screen, but if the password is correct then when you press ENTER, you will be connected.

Change directory to the maintenance directory. For By The Sword Linked, this was:

    cd btsl/mw/maintenance

Make sure the job queue is empty. To check this, use:

    php showJobs.php

To clear the queue, use:

    php runJobs.php --maxjobs 5000

The best number for max jobs may vary by server. Running too many jobs at once can cause problems.

Now run the command to import the XML file. For example:

    php importDump.php --username-prefix="" < aaa-example-01.xml

The script parameter `--username-prefix=""` is needed to make sure that the edits are correctly attributed to the user you specified when you created the XML and are autopatrolled.

For more information about how to run imports, see the [documentation for importDump.php](https://www.mediawiki.org/wiki/Manual:ImportDump.php).

After the import has finished, clear the job queue again.

Several other scripts need to be run to update the database.

Update page counts:

    php initSiteStats.php --update

Update Special:WantedPages:

    php updateSpecialPages.php

If you use the TitleKey extension (as By The Sword Linked did), rebuild the keys to update search suggestions (although the path to the script is due to change from MediaWiki 1.40 onwards):

    php ../extensions/TitleKey/maintenance/rebuildTitleKeys.php

Update Special:RecentChanges:

    php rebuildrecentchanges.php

When you have finished, log off from SSH:

    exit

Then disable SSH.

## Exporting wiki contents

While logged in with SSH, you can export the contents of a wiki using command line scripts.

### Exporting wikitext XML

This can be exported with the maintenance script [dumpBackup.php](https://www.mediawiki.org/wiki/Manual:DumpBackup.php). This can export any number of pages, whereas Special:Export is limited to 4,999 pages.

The commands used for By The Sword Linked exported two files, one for content and one for data structures, filtered by namespace.

Content:

    php dumpBackup.php --current --filter=namespace:0,3000,3002 > by-the-sword-linked-Wikitext-Content.xml

Data structures:

    php dumpBackup.php --current --filter=namespace:4,8,10,12,14,102,106,108 > by-the-sword-linked-Wikitext-Structure.xml

### Exporting RDF

If you are using Semantic MediaWiki, you can also export semantic properties as RDF XML using the maintenance script dumpRDF.php.

The command for By The Sword Linked was (assuming that you are in the MediaWiki maintenance directory):

    cd ../extensions/SemanticMediaWiki/maintenance
    php dumpRDF.php --file by-the-sword-linked-RDF-dump.rdf --individuals -d 50
